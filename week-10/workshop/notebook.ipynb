{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment with Kubernetes and ONNX\n",
    "\n",
    "This notebook covers deploying ML models using Kubernetes, building upon concepts from:\n",
    "- Module 5 - FastAPI deployment\n",
    "- Module 9 - Serverless and ONNX\n",
    "\n",
    "Instead of deploying to AWS Lambda, we'll deploy to a local Kubernetes cluster using Kind (Kubernetes in Docker), making it accessible and cost-free for learners.\n",
    "\n",
    "**Video**: https://www.youtube.com/live/c_CzCsCnWoU?si=fgQ56JwunM3NoiWm\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have:\n",
    "- Docker installed and running\n",
    "- Basic understanding of Python and ML models\n",
    "- Familiarity with FastAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Python Environment Setup\n",
    "\n",
    "Set up the Python environment first so you can use the correct kernel for this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Install uv\n",
    "\n",
    "uv is a fast Python package installer and resolver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting uv\n",
      "  Downloading uv-0.9.16-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Downloading uv-0.9.16-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m22.0/22.0 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: uv\n",
      "Successfully installed uv-0.9.16\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uv 0.9.16\n"
     ]
    }
   ],
   "source": [
    "!uv --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Create Python Environment\n",
    "\n",
    "Create a new Python environment for week-10 using uv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing .venv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 0.63ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m29 packages\u001b[0m \u001b[2min 128ms\u001b[0m\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1masttokens\u001b[0m\u001b[2m==3.0.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mcomm\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mdebugpy\u001b[0m\u001b[2m==1.8.17\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mdecorator\u001b[0m\u001b[2m==5.2.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mexecuting\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mipykernel\u001b[0m\u001b[2m==7.1.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mipython\u001b[0m\u001b[2m==9.8.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mipython-pygments-lexers\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mjedi\u001b[0m\u001b[2m==0.19.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mjupyter-client\u001b[0m\u001b[2m==8.6.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mjupyter-core\u001b[0m\u001b[2m==5.9.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mmatplotlib-inline\u001b[0m\u001b[2m==0.2.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnest-asyncio\u001b[0m\u001b[2m==1.6.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==25.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mparso\u001b[0m\u001b[2m==0.8.5\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpexpect\u001b[0m\u001b[2m==4.9.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mplatformdirs\u001b[0m\u001b[2m==4.5.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mprompt-toolkit\u001b[0m\u001b[2m==3.0.52\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpsutil\u001b[0m\u001b[2m==7.1.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mptyprocess\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpure-eval\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpygments\u001b[0m\u001b[2m==2.19.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpython-dateutil\u001b[0m\u001b[2m==2.9.0.post0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==27.1.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1msix\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mstack-data\u001b[0m\u001b[2m==0.6.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtornado\u001b[0m\u001b[2m==6.5.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtraitlets\u001b[0m\u001b[2m==5.14.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mwcwidth\u001b[0m\u001b[2m==0.2.14\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.13.9\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /workspaces/ml-zoomcamp-works/week-10/workshop\n",
    "[ ! -d \".venv\" ] && uv venv\n",
    "uv sync\n",
    ".venv/bin/python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.3: Register Environment as Jupyter Kernel\n",
    "\n",
    "Register the environment as a Jupyter kernel so you can use it in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m34 packages\u001b[0m \u001b[2min 165ms\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m debugpy \u001b[2m(4.1MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m pygments \u001b[2m(1.2MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m jedi \u001b[2m(1.5MiB)\u001b[0m\n",
      " Downloaded debugpy\n",
      " Downloaded pygments\n",
      " Downloaded jedi\n",
      "\u001b[2mPrepared \u001b[1m27 packages\u001b[0m \u001b[2min 481ms\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m29 packages\u001b[0m \u001b[2min 192ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1masttokens\u001b[0m\u001b[2m==3.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcomm\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdebugpy\u001b[0m\u001b[2m==1.8.17\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdecorator\u001b[0m\u001b[2m==5.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mexecuting\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mipykernel\u001b[0m\u001b[2m==7.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mipython\u001b[0m\u001b[2m==9.8.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mipython-pygments-lexers\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjedi\u001b[0m\u001b[2m==0.19.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjupyter-client\u001b[0m\u001b[2m==8.6.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjupyter-core\u001b[0m\u001b[2m==5.9.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmatplotlib-inline\u001b[0m\u001b[2m==0.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnest-asyncio\u001b[0m\u001b[2m==1.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==25.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mparso\u001b[0m\u001b[2m==0.8.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpexpect\u001b[0m\u001b[2m==4.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mplatformdirs\u001b[0m\u001b[2m==4.5.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprompt-toolkit\u001b[0m\u001b[2m==3.0.52\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpsutil\u001b[0m\u001b[2m==7.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mptyprocess\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpure-eval\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpygments\u001b[0m\u001b[2m==2.19.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dateutil\u001b[0m\u001b[2m==2.9.0.post0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==27.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msix\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mstack-data\u001b[0m\u001b[2m==0.6.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtornado\u001b[0m\u001b[2m==6.5.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtraitlets\u001b[0m\u001b[2m==5.14.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwcwidth\u001b[0m\u001b[2m==0.2.14\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed kernelspec week-10 in /home/codespace/.local/share/jupyter/kernels/week-10\n",
      "Kernel registered. Refresh Jupyter to see it.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /workspaces/ml-zoomcamp-works/week-10/workshop\n",
    "uv pip install ipykernel\n",
    ".venv/bin/python -m ipykernel install --user --name week-10 --display-name \"week-10\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Kubernetes Tools Setup\n",
    "\n",
    "Install Kubernetes command-line tools and set up a local cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Install kubectl\n",
    "\n",
    "kubectl is the Kubernetes command-line tool.\n",
    "\n",
    "**Note**: Installation commands vary by platform:\n",
    "- **Windows (with Chocolatey)**: `choco install kubernetes-cli`\n",
    "- **macOS**: `brew install kubectl`\n",
    "- **Linux**: See commands below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ~/bin\n",
    "cd ~/bin\n",
    "curl -sLO \"https://dl.k8s.io/release/$(curl -Ls https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\n",
    "chmod +x kubectl\n",
    "export PATH=\"${PATH}:${HOME}/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Version: v1.33.2\n",
      "Kustomize Version: v5.6.0\n"
     ]
    }
   ],
   "source": [
    "!export PATH=\"${PATH}:${HOME}/bin\" && kubectl version --client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Install Kind\n",
    "\n",
    "Kind (Kubernetes in Docker) allows you to run Kubernetes clusters locally using Docker containers.\n",
    "\n",
    "**Note**: Installation commands vary by platform:\n",
    "- **Windows (with Chocolatey)**: `choco install kind`\n",
    "- **macOS**: `brew install kind`\n",
    "- **Linux**: See commands below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ~/bin\n",
    "curl -sLo ~/bin/kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64\n",
    "chmod +x ~/bin/kind\n",
    "export PATH=\"${PATH}:${HOME}/bin\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3: Create Kind Cluster\n",
    "\n",
    "Create a local Kubernetes cluster. This will:\n",
    "- Create a single-node Kubernetes cluster (default Kind cluster: 1 control plane node and 0 worker node; you can check by running `kubectl get nodes`) \n",
    "- Configure kubectl to use this cluster\n",
    "- Take a few minutes on first run (downloads images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating cluster \"mlzoomcamp\" ...\n",
      " â€¢ Ensuring node image (kindest/node:v1.27.3) ðŸ–¼  ...\n",
      " âœ“ Ensuring node image (kindest/node:v1.27.3) ðŸ–¼\n",
      " â€¢ Preparing nodes ðŸ“¦   ...\n",
      " âœ“ Preparing nodes ðŸ“¦ \n",
      " â€¢ Writing configuration ðŸ“œ  ...\n",
      " âœ“ Writing configuration ðŸ“œ\n",
      " â€¢ Starting control-plane ðŸ•¹ï¸  ...\n",
      " âœ“ Starting control-plane ðŸ•¹ï¸\n",
      " â€¢ Installing CNI ðŸ”Œ  ...\n",
      " âœ“ Installing CNI ðŸ”Œ\n",
      " â€¢ Installing StorageClass ðŸ’¾  ...\n",
      " âœ“ Installing StorageClass ðŸ’¾\n",
      "Set kubectl context to \"kind-mlzoomcamp\"\n",
      "You can now use your cluster with:\n",
      "\n",
      "kubectl cluster-info --context kind-mlzoomcamp\n",
      "\n",
      "Thanks for using kind! ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PATH=\"${PATH}:${HOME}/bin\"\n",
    "kind create cluster --name mlzoomcamp 2>&1 || true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Verification\n",
    "\n",
    "Verify that everything is set up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kubernetes control plane is running at https://127.0.0.1:35743\n",
      "CoreDNS is running at https://127.0.0.1:35743/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n",
      "\n",
      "To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n",
      "NAME                       STATUS   ROLES           AGE    VERSION\n",
      "mlzoomcamp-control-plane   Ready    control-plane   2m7s   v1.27.3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PATH=\"${PATH}:${HOME}/bin\"\n",
    "kubectl cluster-info\n",
    "kubectl get nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Model Preparation\n",
    "\n",
    "Download the pre-trained ONNX model for clothing classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained PyTorch model converted to ONNX format. Predicts one of 10 clothing categories from an image URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /workspaces/ml-zoomcamp-works/week-10/workshop/service\n",
    "wget -q https://github.com/DataTalksClub/machine-learning-zoomcamp/releases/download/dl-models/clothing_classifier_mobilenet_v2_latest.onnx -O clothing-model.onnx\n",
    "ls -lh clothing-model.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Building the FastAPI Service\n",
    "\n",
    "Create a FastAPI application that serves the ONNX model for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.1: Initialize Project\n",
    "\n",
    "Initialize the service project and add dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /workspaces/ml-zoomcamp-works/week-10/workshop/service\n",
    "[ ! -f pyproject.toml ] && uv init\n",
    "rm -f main.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /workspaces/ml-zoomcamp-works/week-10/workshop/service\n",
    "rm -f uv.lock\n",
    "uv add fastapi uvicorn onnxruntime keras-image-helper numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.2: FastAPI Application\n",
    "\n",
    "Create the FastAPI application that uses ONNX Runtime for inference and includes health checks for Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create FastAPI Application\n",
    "\n",
    "Create the FastAPI application file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /workspaces/ml-zoomcamp-works/week-10/workshop/service/app.py\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from keras_image_helper import create_preprocessor\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel, HttpUrl\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI(title=\"clothing-classifier\")\n",
    "\n",
    "def preprocess_pytorch_style(X):\n",
    "    X = X / 255.0\n",
    "    \n",
    "    mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3, 1, 1)\n",
    "    std = np.array([0.229, 0.224, 0.225]).reshape(1, 3, 1, 1)\n",
    "    \n",
    "    X = X.transpose(0, 3, 1, 2)\n",
    "    X = (X - mean) / std\n",
    "    \n",
    "    return X.astype(np.float32)\n",
    "\n",
    "preprocessor = create_preprocessor(\n",
    "    preprocess_pytorch_style,\n",
    "    target_size=(224, 224)\n",
    ")\n",
    "\n",
    "# Load the ONNX model using ONNX Runtime\n",
    "session = ort.InferenceSession(\n",
    "    \"clothing-model.onnx\",\n",
    "    providers=[\"CPUExecutionProvider\"]  # Run on CPU\n",
    ")\n",
    "\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "\n",
    "classes = [\n",
    "    \"dress\",\n",
    "    \"hat\",\n",
    "    \"longsleeve\",\n",
    "    \"outwear\",\n",
    "    \"pants\",\n",
    "    \"shirt\",\n",
    "    \"shoes\",\n",
    "    \"shorts\",\n",
    "    \"skirt\",\n",
    "    \"t-shirt\",\n",
    "]\n",
    "\n",
    "\n",
    "# Pydantic model for incoming request\n",
    "# Ensures \"url\" is a valid URL string\n",
    "class PredictRequest(BaseModel):\n",
    "    url: HttpUrl\n",
    "\n",
    "\n",
    "# Pydantic response model\n",
    "# Ensures API returns structured, validated JSON\n",
    "class PredictResponse(BaseModel):\n",
    "    predictions: dict[str, float]\n",
    "    top_class: str\n",
    "    top_probability: float\n",
    "\n",
    "\n",
    "def predict(url: str):\n",
    "    X = preprocessor.from_url(url)\n",
    "    result = session.run([output_name], {input_name: X})\n",
    "    float_predictions = result[0][0].tolist()\n",
    "    predictions_dict = dict(zip(classes, float_predictions))\n",
    "    \n",
    "    top_class = max(predictions_dict, key=predictions_dict.get)\n",
    "    top_probability = predictions_dict[top_class]\n",
    "    \n",
    "    return predictions_dict, top_class, top_probability\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\"message\": \"Clothing Classification Service\"}\n",
    "\n",
    "# Health check endpoint - for monitoring / Kubernetes probes\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "# Main prediction API endpoint\n",
    "# - Accepts JSON body with image URL\n",
    "# - Returns probabilities and top class\n",
    "@app.post(\"/predict\", response_model=PredictResponse)\n",
    "def predict_endpoint(request: PredictRequest):\n",
    "    predictions, top_class, top_prob = predict(str(request.url))\n",
    "    \n",
    "    return PredictResponse(\n",
    "        predictions=predictions,\n",
    "        top_class=top_class,\n",
    "        top_probability=top_prob\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Test Script\n",
    "\n",
    "Create a test script to verify the service works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /workspaces/ml-zoomcamp-works/week-10/workshop/service/test.py\n",
    "import requests\n",
    "\n",
    "url = 'http://localhost:8080/predict'\n",
    "\n",
    "request = {\n",
    "    \"url\": \"http://bit.ly/mlbookcamp-pants\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=request)\n",
    "result = response.json()\n",
    "\n",
    "print(f\"Top prediction: {result['top_class']} ({result['top_probability']:.2%})\")\n",
    "print(f\"\\nAll predictions:\")\n",
    "for cls, prob in result['predictions'].items():\n",
    "    print(f\"  {cls:12s}: {prob:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.3: Test Locally\n",
    "\n",
    "Run the service and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /workspaces/ml-zoomcamp-works/week-10/workshop/service\n",
    "uv run uvicorn app:app --host 0.0.0.0 --port 8080 --reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Start the service above, then run the test below in a separate terminal or after stopping the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /workspaces/ml-zoomcamp-works/week-10/workshop/service\n",
    "uv run python test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Docker Containerization\n",
    "\n",
    "Containerize the application with Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.1: Create Dockerfile\n",
    "\n",
    "Create a Dockerfile to containerize the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /workspaces/ml-zoomcamp-works/week-10/workshop/service/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile /workspaces/ml-zoomcamp-works/week-10/workshop/service/Dockerfile\n",
    "FROM python:3.13.5-slim-bookworm\n",
    "\n",
    "COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "ENV PATH=\"/app/.venv/bin:$PATH\"\n",
    "\n",
    "COPY pyproject.toml README.md .python-version ./\n",
    "RUN uv sync\n",
    "\n",
    "COPY app.py clothing-model.onnx ./\n",
    "\n",
    "EXPOSE 8080\n",
    "\n",
    "ENTRYPOINT [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.2: Build Docker Image\n",
    "\n",
    "Build the Docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /workspaces/ml-zoomcamp-works/week-10/workshop/service\n",
    "docker build -t clothing-classifier:v1 .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.3: Test Container\n",
    "\n",
    "Run the container and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /workspaces/ml-zoomcamp-works/week-10/workshop/service\n",
    "docker run -d --rm -p 8080:8080 --name clothing-classifier clothing-classifier:v1\n",
    "sleep 3\n",
    "curl -s http://localhost:8080/health\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /workspaces/ml-zoomcamp-works/week-10/workshop/service\n",
    "uv run python test.py\n",
    "docker stop clothing-classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Loading Image to Kind\n",
    "\n",
    "Load the Docker image into the Kind cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kind clusters run in Docker, so they can't access images from your local Docker daemon by default. Load the image into Kind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image: \"clothing-classifier:v1\" with ID \"sha256:28de0f9e811509097ed088f3be093dc9f218a9e391d3fbc4b9e342137a7e91ee\" not yet present on node \"mlzoomcamp-control-plane\", loading...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PATH=\"${PATH}:${HOME}/bin\"\n",
    "kind load docker-image clothing-classifier:v1 --name mlzoomcamp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Kubernetes Deployment\n",
    "\n",
    "Deploy the service to Kubernetes.\n",
    "\n",
    "### Understanding Kubernetes Resources\n",
    "\n",
    "- **Pod**: The smallest deployable unit in Kubernetes (one or more containers)\n",
    "- **Deployment**: Manages a set of identical Pods, handles updates and scaling\n",
    "- **Service**: Exposes Pods to network traffic\n",
    "- **HPA (Horizontal Pod Autoscaler)**: Automatically scales Pods based on metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8.1: Create Deployment\n",
    "\n",
    "Create the Deployment manifest and deploy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /workspaces/ml-zoomcamp-works/week-10/workshop/k8s/deployment.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile /workspaces/ml-zoomcamp-works/week-10/workshop/k8s/deployment.yaml\n",
    "\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "\n",
    "metadata:\n",
    "  name: clothing-classifier              # Name of the deployment\n",
    "  labels:\n",
    "    app: clothing-classifier\n",
    "\n",
    "spec:\n",
    "  replicas: 2                            # Run 2 identical pods\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: clothing-classifier           # Pods must have this label\n",
    "\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: clothing-classifier         # Label added to each pod\n",
    "\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: clothing-classifier\n",
    "        image: clothing-classifier:v1    # Docker image to run\n",
    "        imagePullPolicy: Never           # Use local image (Kind)\n",
    "\n",
    "        ports:\n",
    "        - containerPort: 8080            # App listens on port 8080\n",
    "\n",
    "        resources:\n",
    "          requests:                      # Minimum resources reserved\n",
    "            memory: \"256Mi\"\n",
    "            cpu: \"250m\"\n",
    "          limits:                        # Maximum allowed resources\n",
    "            memory: \"512Mi\"\n",
    "            cpu: \"500m\"\n",
    "\n",
    "        # Kubernetes restarts the pod if this probe fails\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8080\n",
    "          initialDelaySeconds: 10\n",
    "          periodSeconds: 10\n",
    "\n",
    "        # Pod receives traffic only when this probe succeeds\n",
    "        readinessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8080\n",
    "          initialDelaySeconds: 5\n",
    "          periodSeconds: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "\n",
    "| Setting                         | Meaning                                                 |\n",
    "| ------------------------------- | ------------------------------------------------------- |\n",
    "| `imagePullPolicy: Always`       | Always pull from registry â†’ fails in Kind               |\n",
    "| `imagePullPolicy: IfNotPresent` | Pull only if missing locally                            |\n",
    "| `imagePullPolicy: Never`        | **Never pull; only use local image** â†’ correct for Kind |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deploy the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps/clothing-classifier created\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PATH=\"${PATH}:${HOME}/bin\"\n",
    "kubectl apply -f /workspaces/ml-zoomcamp-works/week-10/workshop/k8s/deployment.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                  READY   UP-TO-DATE   AVAILABLE   AGE\n",
      "clothing-classifier   2/2     2            2           15s\n",
      "NAME                                   READY   STATUS    RESTARTS   AGE\n",
      "clothing-classifier-6fbf5bc4d4-cbmf5   1/1     Running   0          15s\n",
      "clothing-classifier-6fbf5bc4d4-dvb7r   1/1     Running   0          15s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PATH=\"${PATH}:${HOME}/bin\"\n",
    "kubectl get deployments\n",
    "kubectl get pods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "\n",
    "| Column         | Explanation                                                      |\n",
    "| -------------- | ---------------------------------------------------------------- |\n",
    "| **NAME**       | Name of your Deployment (`clothing-classifier`)                  |\n",
    "| **READY**      | `2/2` â†’ 2 pods are running and ready (matching desired replicas) |\n",
    "| **UP-TO-DATE** | 2 pods have the latest version of your Deployment spec           |\n",
    "| **AVAILABLE**  | 2 pods are available to serve traffic                            |\n",
    "| **AGE**        | Deployment was created 15 seconds ago                            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8.2: Create Service\n",
    "\n",
    "Create the Service manifest and deploy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /workspaces/ml-zoomcamp-works/week-10/workshop/k8s/service.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile /workspaces/ml-zoomcamp-works/week-10/workshop/k8s/service.yaml\n",
    "\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "\n",
    "metadata:\n",
    "  name: clothing-classifier          # Service name\n",
    "\n",
    "spec:\n",
    "  type: NodePort                     # Expose service on a port of each node\n",
    "  selector:\n",
    "    app: clothing-classifier         # Route traffic to pods with this label\n",
    "\n",
    "  ports:\n",
    "  - protocol: TCP\n",
    "    port: 8080                       # Service port (inside cluster)\n",
    "    targetPort: 8080                 # Container port\n",
    "    nodePort: 30080                  # Port exposed externally on the node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deploy the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service/clothing-classifier created\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PATH=\"${PATH}:${HOME}/bin\"\n",
    "kubectl apply -f /workspaces/ml-zoomcamp-works/week-10/workshop/k8s/service.yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\n",
      "clothing-classifier   NodePort    10.96.174.215   <none>        8080:30080/TCP   28s\n",
      "kubernetes            ClusterIP   10.96.0.1       <none>        443/TCP          112m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PATH=\"${PATH}:${HOME}/bin\"\n",
    "kubectl get services\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "\n",
    "| Column          | Meaning                                                              | Example from Your Output                 |\n",
    "| --------------- | -------------------------------------------------------------------- | ---------------------------------------- |\n",
    "| **NAME**        | The name of the Service                                              | `clothing-classifier`                    |\n",
    "| **TYPE**        | How the Service is exposed (`ClusterIP`, `NodePort`, `LoadBalancer`) | `NodePort` â†’ reachable through nodeâ€™s IP |\n",
    "| **CLUSTER-IP**  | Internal virtual IP used *inside the cluster*                        | `10.96.174.215`                          |\n",
    "| **EXTERNAL-IP** | Public IP (for cloud LoadBalancer) or `<none>` if not applicable     | `<none>` (expected for NodePort)         |\n",
    "| **PORT(S)**     | `<service-port>:<node-port>/protocol`                                | `8080:30080/TCP`                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also [Understanding Kubernetes Services Through the Nobi House Analogy](docs/Understanding_Kubernetes_Server.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8.3: Test Deployed Service\n",
    "\n",
    "Use port-forward to access the service (NodePort doesn't work with Kind)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you try\n",
    "\n",
    "```\n",
    "curl http://localhost:30080/health\n",
    "```\n",
    "\n",
    "This assumes that **NodePort works** in your Kubernetes cluster and that port **30080** is reachable from your machine.\n",
    "\n",
    "But in Kind, NodePort does *not* work the way you expect. Kind runs Kubernetes **inside Docker containers**.\n",
    "So each Kubernetes â€œnodeâ€ is actually a Docker container and NodePorts open ports **inside the node container**, not on your host machine. That means:\n",
    "\n",
    "* `localhost:30080` on your laptop â‰  port 30080 inside the Kind node\n",
    "* Therefore your curl request fails\n",
    "\n",
    "NodePort *can* work in Kind, but only if the cluster is created with special port mappings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since NodePort is not accessible from outside Kind, we â€œtunnelâ€ traffic using port-forward:\n",
    "\n",
    "```\n",
    "kubectl port-forward service/clothing-classifier 30080:8080\n",
    "```\n",
    "\n",
    "This means:\n",
    "\n",
    "| Local machine       | Port-forward | Pod            |\n",
    "| ------------------- | ------------ | -------------- |\n",
    "| `localhost:30080` â†’ | forwarded to | `service:8080` |\n",
    "\n",
    "So Kubernetes **forwards** local traffic to the service inside the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status\":\"healthy\"}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PATH=\"${PATH}:${HOME}/bin\"\n",
    "\n",
    "# Stop any existing port-forward process for this service (if running)\n",
    "# pkill exits with error if no process exists, so \"|| true\" avoids breaking the script\n",
    "pkill -f 'kubectl port-forward.*clothing-classifier' || true\n",
    "sleep 1\n",
    "\n",
    "# Start a new port-forward in the background\n",
    "# - Forwards localhost:30080 â†’ service/clothing-classifier:8080\n",
    "# - Redirects output to /dev/null to keep the notebook clean\n",
    "kubectl port-forward service/clothing-classifier 30080:8080 > /dev/null 2>&1 &\n",
    "sleep 2\n",
    "curl -s http://localhost:30080/health\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Horizontal Pod Autoscaling\n",
    "\n",
    "Automatically scale Pods based on CPU usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9.1: Install Metrics Server\n",
    "\n",
    "First, we need metrics-server for HPA to work. Install it in kubectl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/metrics-server created\n",
      "clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created\n",
      "clusterrole.rbac.authorization.k8s.io/system:metrics-server created\n",
      "rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created\n",
      "service/metrics-server created\n",
      "deployment.apps/metrics-server created\n",
      "apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PATH=\"${PATH}:${HOME}/bin\"\n",
    "kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: See [Metrics Server (Kubernetes)](docs/Metrics_Server_(Kubernetes).md) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Kind, we need to patch metrics-server to work without TLS. See [Why Metrics Server Needs a Patch in Kind](docs/Metrics_Server_Patch_in_Kind.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps/metrics-server patched\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PATH=\"${PATH}:${HOME}/bin\"\n",
    "kubectl patch -n kube-system deployment metrics-server --type=json -p '[{\"op\":\"add\",\"path\":\"/spec/template/spec/containers/0/args/-\",\"value\":\"--kubelet-insecure-tls\"}]'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for metrics-server to be ready:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME             READY   UP-TO-DATE   AVAILABLE   AGE\n",
      "metrics-server   1/1     1            1           26s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PATH=\"${PATH}:${HOME}/bin\"\n",
    "kubectl get deployment metrics-server -n kube-system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9.2: Create HPA\n",
    "\n",
    "Create and deploy the Horizontal Pod Autoscaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /workspaces/ml-zoomcamp-works/week-10/workshop/k8s/hpa.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile /workspaces/ml-zoomcamp-works/week-10/workshop/k8s/hpa.yaml\n",
    "\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "\n",
    "metadata:\n",
    "  name: clothing-classifier-hpa         # Name of the HPA resource\n",
    "\n",
    "spec:\n",
    "  scaleTargetRef:                       # The workload the HPA will scale\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: clothing-classifier           # Scale this Deployment's pods\n",
    "\n",
    "  minReplicas: 2                        # Never scale below 2 pods\n",
    "  maxReplicas: 10                       # Upper limit: up to 10 pods\n",
    "\n",
    "  metrics:\n",
    "  - type: Resource                      # Use resource-based metrics (CPU)\n",
    "    resource:\n",
    "      name: cpu                         # Monitor CPU usage\n",
    "      target:\n",
    "        type: Utilization               # Target CPU % of each pod\n",
    "        averageUtilization: 50          # Scale up if avg CPU > 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy HPA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horizontalpodautoscaler.autoscaling/clothing-classifier-hpa created\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PATH=\"${PATH}:${HOME}/bin\"\n",
    "kubectl apply -f /workspaces/ml-zoomcamp-works/week-10/workshop/k8s/hpa.yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check HPA status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                      REFERENCE                        TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\n",
      "clothing-classifier-hpa   Deployment/clothing-classifier   0%/50%    2         10        2          16s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PATH=\"${PATH}:${HOME}/bin\"\n",
    "kubectl get hpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9.3: Test Autoscaling\n",
    "\n",
    "Generate load to trigger autoscaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /workspaces/ml-zoomcamp-works/week-10/workshop/load_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /workspaces/ml-zoomcamp-works/week-10/workshop/load_test.py\n",
    "import time\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Endpoint exposed via port-forward or NodePort\n",
    "url = 'http://localhost:30080/predict'\n",
    "\n",
    "# Payload sent with each request\n",
    "request_data = {\n",
    "    \"url\": \"http://bit.ly/mlbookcamp-pants\"\n",
    "}\n",
    "\n",
    "# Function executed by each worker thread\n",
    "def send_request(_):\n",
    "    try:\n",
    "        response = requests.post(url, json=request_data, timeout=5)\n",
    "        return response.status_code\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "print(\"Starting load test...\")\n",
    "print(\"Watch HPA with: kubectl get hpa -w\")\n",
    "print(\"Watch pods with: kubectl get pods -w\")\n",
    "\n",
    "num_requests = 1000          # Total number of requests to send\n",
    "concurrent_workers = 50      # Number of threads sending requests in parallel\n",
    "\n",
    "print(f\"Sending {num_requests} requests with {concurrent_workers} concurrent workers\")\n",
    "print()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Launch concurrent requests using a thread pool\n",
    "with ThreadPoolExecutor(max_workers=concurrent_workers) as executor:\n",
    "    futures = [executor.submit(send_request, i) for i in range(num_requests)]\n",
    "    results = [future.result() for future in as_completed(futures)]\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "# Count how many requests succeeded (HTTP 200)\n",
    "success_count = sum(1 for r in results if r == 200)\n",
    "error_count = len(results) - success_count\n",
    "\n",
    "print(\"Load test complete!\")\n",
    "print(f\"Duration: {duration:.2f} seconds\")\n",
    "print(f\"Requests per second: {len(results)/duration:.2f}\")\n",
    "print(f\"Successful requests: {success_count}\")\n",
    "print(f\"Failed requests: {error_count}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, check that you can access the endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PATH=\"${PATH}:${HOME}/bin\"\n",
    "\n",
    "# Stop any existing port-forward process for this service (if running)\n",
    "# pkill exits with error if no process exists, so \"|| true\" avoids breaking the script\n",
    "pkill -f 'kubectl port-forward.*clothing-classifier' || true\n",
    "sleep 1\n",
    "\n",
    "# Start a new port-forward in the background\n",
    "# - Forwards localhost:30080 â†’ service/clothing-classifier:8080\n",
    "# - Redirects output to /dev/null to keep the notebook clean\n",
    "kubectl port-forward service/clothing-classifier 30080:8080 > /dev/null 2>&1 &\n",
    "sleep 2\n",
    "curl -s http://localhost:30080/health\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /workspaces/ml-zoomcamp-works/week-10/workshop\n",
    "uv add requests\n",
    "uv run python load_test.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** While the load test runs, watch HPA and pods in separate terminals:\n",
    "- `kubectl get hpa -w`\n",
    "- `kubectl get pods -w`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Managing the Deployment\n",
    "\n",
    "Common operations for managing the Kubernetes deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10.1: Update Application\n",
    "\n",
    "Rebuild and update the deployment with a new image version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /workspaces/ml-zoomcamp-works/week-10/workshop/service\n",
    "docker build -t clothing-classifier:v2 .\n",
    "export PATH=\"${PATH}:${HOME}/bin\"\n",
    "kind load docker-image clothing-classifier:v2 --name mlzoomcamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PATH=\"${PATH}:${HOME}/bin\"\n",
    "kubectl set image deployment/clothing-classifier clothing-classifier=clothing-classifier:v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PATH=\"${PATH}:${HOME}/bin\"\n",
    "kubectl rollout status deployment/clothing-classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10.2: Manual Scaling\n",
    "\n",
    "Manually scale the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PATH=\"${PATH}:${HOME}/bin\"\n",
    "kubectl scale deployment clothing-classifier --replicas=5\n",
    "kubectl get pods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10.3: View Logs\n",
    "\n",
    "View logs from the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PATH=\"${PATH}:${HOME}/bin\"\n",
    "kubectl logs -l app=clothing-classifier --tail=20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10.4: Debugging\n",
    "\n",
    "Debug deployment issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PATH=\"${PATH}:${HOME}/bin\"\n",
    "kubectl describe deployment clothing-classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PATH=\"${PATH}:${HOME}/bin\"\n",
    "kubectl get events --sort-by='.lastTimestamp' | tail -10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Cleanup\n",
    "\n",
    "Clean up Docker images and Kubernetes resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11.1: Cleanup Kubernetes Resources\n",
    "\n",
    "Delete deployments, services, and HPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps \"clothing-classifier\" deleted from default namespace\n",
      "service \"clothing-classifier\" deleted from default namespace\n",
      "horizontalpodautoscaler.autoscaling \"clothing-classifier-hpa\" deleted from default namespace\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PATH=\"${PATH}:${HOME}/bin\"\n",
    "kubectl delete -f /workspaces/ml-zoomcamp-works/week-10/workshop/k8s/deployment.yaml\n",
    "kubectl delete -f /workspaces/ml-zoomcamp-works/week-10/workshop/k8s/service.yaml\n",
    "kubectl delete -f /workspaces/ml-zoomcamp-works/week-10/workshop/k8s/hpa.yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop any existing `kubectl port-forward` process for the `clothing-classifier` service so a new one can start without conflicts:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PATH=\"${PATH}:${HOME}/bin\"\n",
    "pkill -f 'kubectl port-forward.*clothing-classifier' || true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11.2: Cleanup Docker Images\n",
    "\n",
    "Remove Docker images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untagged: clothing-classifier:v1\n",
      "Deleted: sha256:28de0f9e811509097ed088f3be093dc9f218a9e391d3fbc4b9e342137a7e91ee\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error response from daemon: No such image: clothing-classifier:v2\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker rmi clothing-classifier:v1 clothing-classifier:v2 || true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11.3: Cleanup Kind Cluster (Optional)\n",
    "\n",
    "Delete the Kind cluster if you want to start fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PATH=\"${PATH}:${HOME}/bin\"\n",
    "# Uncomment the line below to delete the cluster:\n",
    "# kind delete cluster --name mlzoomcamp\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (week-10)",
   "language": "python",
   "name": "week-10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
