{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine Learning and Deep Learning Model Deployment with Serverless\n",
        "\n",
        "This notebook covers deploying ML models using AWS Lambda and Docker, including:\n",
        "- Scikit-Learn model deployment\n",
        "- Deep Learning models with ONNX (TensorFlow/Keras and PyTorch)\n",
        "\n",
        "**Video**: https://www.youtube.com/watch?v=sHQaeVm5hT8\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- AWS Account\n",
        "- AWS CLI installed and configured\n",
        "- Docker installed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Training a Scikit-Learn Model\n",
        "\n",
        "First, we'll train a simple churn prediction model using Scikit-Learn. This model will be deployed to AWS Lambda in the following sections.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pandas==2.3.3\n",
            "sklearn==1.6.1\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "\n",
        "print(f'pandas=={pd.__version__}')\n",
        "print(f'sklearn=={sklearn.__version__}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def load_data():\n",
        "#     \"\"\"\n",
        "#     Loads and preprocesses the Telco customer churn dataset.\n",
        "    \n",
        "#     Returns:\n",
        "#         pd.DataFrame: Preprocessed dataframe with cleaned column names and data types.\n",
        "#     \"\"\"\n",
        "#     data_url = 'https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-03-churn-prediction/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
        "\n",
        "#     df = pd.read_csv(data_url)\n",
        "\n",
        "#     # Normalize column names\n",
        "#     df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
        "\n",
        "#     # Normalize categorical values\n",
        "#     categorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n",
        "\n",
        "#     for column in categorical_columns:\n",
        "#         df[column] = df[column].str.lower().str.replace(' ', '_')\n",
        "\n",
        "#     # Handle numeric conversion and missing values\n",
        "#     df.totalcharges = pd.to_numeric(df.totalcharges, errors='coerce')\n",
        "#     df.totalcharges = df.totalcharges.fillna(0)\n",
        "\n",
        "#     # Convert churn to binary\n",
        "#     df.churn = (df.churn == 'yes').astype(int)\n",
        "    \n",
        "#     return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def train_model(df):\n",
        "#     \"\"\"\n",
        "#     Trains a logistic regression model for churn prediction.\n",
        "    \n",
        "#     Args:\n",
        "#         df (pd.DataFrame): Preprocessed dataframe with features and target.\n",
        "    \n",
        "#     Returns:\n",
        "#         sklearn.pipeline.Pipeline: Trained pipeline with DictVectorizer and LogisticRegression.\n",
        "#     \"\"\"\n",
        "#     numerical_features = ['tenure', 'monthlycharges', 'totalcharges']\n",
        "\n",
        "#     categorical_features = [\n",
        "#         'gender',\n",
        "#         'seniorcitizen',\n",
        "#         'partner',\n",
        "#         'dependents',\n",
        "#         'phoneservice',\n",
        "#         'multiplelines',\n",
        "#         'internetservice',\n",
        "#         'onlinesecurity',\n",
        "#         'onlinebackup',\n",
        "#         'deviceprotection',\n",
        "#         'techsupport',\n",
        "#         'streamingtv',\n",
        "#         'streamingmovies',\n",
        "#         'contract',\n",
        "#         'paperlessbilling',\n",
        "#         'paymentmethod',\n",
        "#     ]\n",
        "\n",
        "#     y_train = df.churn\n",
        "#     train_dict = df[categorical_features + numerical_features].to_dict(orient='records')\n",
        "\n",
        "#     pipeline = make_pipeline(\n",
        "#         DictVectorizer(),\n",
        "#         LogisticRegression(solver='liblinear')\n",
        "#     )\n",
        "\n",
        "#     pipeline.fit(train_dict, y_train)\n",
        "\n",
        "#     return pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def save_model(pipeline, output_file):\n",
        "#     \"\"\"\n",
        "#     Saves a trained pipeline to disk using pickle.\n",
        "    \n",
        "#     Args:\n",
        "#         pipeline: Trained sklearn pipeline to save.\n",
        "#         output_file (str): Path where the model will be saved.\n",
        "#     \"\"\"\n",
        "#     with open(output_file, 'wb') as f_out:\n",
        "#         pickle.dump(pipeline, f_out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Load data and train model\n",
        "# df = load_data()\n",
        "# pipeline = train_model(df)\n",
        "# save_model(pipeline, 'model.bin')\n",
        "\n",
        "# print('Model saved to model.bin')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: AWS Lambda Basics\n",
        "\n",
        "AWS Lambda is a serverless compute service that runs code in response to events. Let's start with a simple Lambda function that returns a mock prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Invoking the Lambda Function\n",
        "\n",
        "Once deployed, you can invoke the Lambda function in several ways:\n",
        "\n",
        "#### Method 1: AWS CLI\n",
        "\n",
        "First, create a JSON file with the customer data (e.g., `customer.json`):\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"customer\": {\n",
        "    \"gender\": \"female\",\n",
        "    \"seniorcitizen\": 0,\n",
        "    \"partner\": \"yes\",\n",
        "    \"dependents\": \"no\",\n",
        "    \"phoneservice\": \"no\",\n",
        "    \"multiplelines\": \"no_phone_service\",\n",
        "    \"internetservice\": \"dsl\",\n",
        "    \"onlinesecurity\": \"no\",\n",
        "    \"onlinebackup\": \"yes\",\n",
        "    \"deviceprotection\": \"no\",\n",
        "    \"techsupport\": \"no\",\n",
        "    \"streamingtv\": \"no\",\n",
        "    \"streamingmovies\": \"no\",\n",
        "    \"contract\": \"month-to-month\",\n",
        "    \"paperlessbilling\": \"yes\",\n",
        "    \"paymentmethod\": \"electronic_check\",\n",
        "    \"tenure\": 1,\n",
        "    \"monthlycharges\": 29.85,\n",
        "    \"totalcharges\": 29.85\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "Then invoke the function:\n",
        "\n",
        "```bash\n",
        "aws lambda invoke --function-name churn_prediction --cli-binary-format raw-in-base64-out --payload file://customer.json --region us-west-2 output.json && cat output.json\n",
        "```\n",
        "\n",
        "**Note:** Make sure to specify the `--region` parameter matching the region where your Lambda function is deployed. If you haven't configured AWS CLI defaults, you can also set the region using `aws configure` or by setting the `AWS_DEFAULT_REGION` environment variable.\n",
        "\n",
        "The response will be saved to `output.json`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Method 2: Using boto3 (Python)\n",
        "\n",
        "You can also invoke the Lambda function programmatically using boto3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Using `aws login` credentials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code below uses `aws login` credentials. For a simpler alternative, use `aws configure` instead - then boto3 will work automatically without credential loading code. See [troubleshooting guide](aws-docs/troubleshooting.md) for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ AWS credentials loaded from aws login cache\n"
          ]
        }
      ],
      "source": [
        "# import boto3\n",
        "# import json\n",
        "# import os\n",
        "# from pathlib import Path\n",
        "\n",
        "\n",
        "# def load_aws_login_credentials():\n",
        "#     \"\"\"\n",
        "#     Loads credentials from aws login cache.\n",
        "    \n",
        "#     Returns:\n",
        "#         dict: Credentials dict with access_key, secret_key, token, or None if not found.\n",
        "#     \"\"\"\n",
        "#     login_cache_dir = Path.home() / '.aws' / 'login' / 'cache'\n",
        "#     credential_files = list(login_cache_dir.glob('*.json'))\n",
        "    \n",
        "#     if not credential_files:\n",
        "#         return None\n",
        "    \n",
        "#     try:\n",
        "#         with open(credential_files[0]) as f:\n",
        "#             creds_data = json.load(f)\n",
        "#             access_token = creds_data.get('accessToken', {})\n",
        "#             return {\n",
        "#                 'aws_access_key_id': access_token.get('accessKeyId'),\n",
        "#                 'aws_secret_access_key': access_token.get('secretAccessKey'),\n",
        "#                 'aws_session_token': access_token.get('sessionToken')\n",
        "#             }\n",
        "#     except Exception:\n",
        "#         return None\n",
        "\n",
        "\n",
        "# # Load credentials once at module level\n",
        "# _aws_creds = load_aws_login_credentials()\n",
        "# if _aws_creds:\n",
        "#     os.environ.update(_aws_creds)\n",
        "#     print(\"✓ AWS credentials loaded from aws login cache\")\n",
        "# else:\n",
        "#     print(\"⚠ Warning: No credentials found in aws login cache. Make sure you've run 'aws login'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def invoke_lambda_function(function_name, payload, region_name='us-west-2'):\n",
        "#     \"\"\"\n",
        "#     Invokes an AWS Lambda function.\n",
        "    \n",
        "#     Args:\n",
        "#         function_name (str): Lambda function name.\n",
        "#         payload (dict): Request payload.\n",
        "#         region_name (str): AWS region. Defaults to 'us-west-2'.\n",
        "    \n",
        "#     Returns:\n",
        "#         dict: Lambda function response.\n",
        "#     \"\"\"\n",
        "#     if _aws_creds:\n",
        "#         session = boto3.Session(\n",
        "#             aws_access_key_id=_aws_creds['aws_access_key_id'],\n",
        "#             aws_secret_access_key=_aws_creds['aws_secret_access_key'],\n",
        "#             aws_session_token=_aws_creds['aws_session_token'],\n",
        "#             region_name=region_name\n",
        "#         )\n",
        "#     else:\n",
        "#         session = boto3.Session(region_name=region_name)\n",
        "    \n",
        "#     lambda_client = session.client('lambda', region_name=region_name)\n",
        "#     response = lambda_client.invoke(\n",
        "#         FunctionName=function_name,\n",
        "#         InvocationType='RequestResponse',\n",
        "#         Payload=json.dumps(payload)\n",
        "#     )\n",
        "#     return json.loads(response['Payload'].read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Alternative: Using `aws configure` (Simpler)\n",
        "\n",
        "If you use `aws configure` instead of `aws login`, boto3 works automatically without credential loading code:\n",
        "\n",
        "**Setup:** Run `aws configure` in terminal, then boto3 automatically uses credentials from `~/.aws/credentials`. See [troubleshooting guide](aws-docs/troubleshooting.md) for details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import boto3\n",
        "import json\n",
        "\n",
        "def invoke_lambda_function(function_name, payload, region_name='us-east-1'):\n",
        "    \"\"\"\n",
        "    Invokes an AWS Lambda function.\n",
        "    \n",
        "    Args:\n",
        "        function_name (str): Lambda function name.\n",
        "        payload (dict): Request payload.\n",
        "        region_name (str): AWS region. Defaults to 'us-west-2'.\n",
        "    \n",
        "    Returns:\n",
        "        dict: Lambda function response.\n",
        "    \"\"\"\n",
        "    lambda_client = boto3.client('lambda', region_name=region_name)\n",
        "    response = lambda_client.invoke(\n",
        "        FunctionName=function_name,\n",
        "        InvocationType='RequestResponse',\n",
        "        Payload=json.dumps(payload)\n",
        "    )\n",
        "    return json.loads(response['Payload'].read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"churn_probability\": 0.56,\n",
            "  \"churn\": true\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "customer_data = {\n",
        "    \"customer\": {\n",
        "        \"gender\": \"female\",\n",
        "        \"seniorcitizen\": 0,\n",
        "        \"partner\": \"yes\",\n",
        "        \"dependents\": \"no\",\n",
        "        \"phoneservice\": \"no\",\n",
        "        \"multiplelines\": \"no_phone_service\",\n",
        "        \"internetservice\": \"dsl\",\n",
        "        \"onlinesecurity\": \"no\",\n",
        "        \"onlinebackup\": \"yes\",\n",
        "        \"deviceprotection\": \"no\",\n",
        "        \"techsupport\": \"no\",\n",
        "        \"streamingtv\": \"no\",\n",
        "        \"streamingmovies\": \"no\",\n",
        "        \"contract\": \"month-to-month\",\n",
        "        \"paperlessbilling\": \"yes\",\n",
        "        \"paymentmethod\": \"electronic_check\",\n",
        "        \"tenure\": 1,\n",
        "        \"monthlycharges\": 29.85,\n",
        "        \"totalcharges\": 29.85\n",
        "    }\n",
        "}\n",
        "\n",
        "result = invoke_lambda_function('churn_prediction', customer_data, region_name='us-east-1')\n",
        "print(json.dumps(result, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** You can also expose the Lambda function as a web service using API Gateway. See [unit 9.7 about API Gateway](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/09-serverless/07-api-gateway.md) for more details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: AWS Lambda with Docker\n",
        "\n",
        "Scikit-Learn and its dependencies exceed the 250MB ZIP archive limit for Lambda. Docker containers solve this problem.\n",
        "\n",
        "### Loading the Model\n",
        "\n",
        "To use the actual model, we need to load it:\n",
        "\n",
        "```python\n",
        "import pickle\n",
        "\n",
        "with open('model.bin', 'rb') as f_in:\n",
        "    pipeline = pickle.load(f_in)\n",
        "\n",
        "def predict_single(customer):\n",
        "    result = pipeline.predict_proba(customer)[0, 1]\n",
        "    return float(result)\n",
        "```\n",
        "\n",
        "### Dockerfile\n",
        "\n",
        "Create a `Dockerfile` based on AWS Lambda Python base image:\n",
        "\n",
        "```dockerfile\n",
        "FROM public.ecr.aws/lambda/python:3.13\n",
        "COPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/\n",
        "\n",
        "COPY pyproject.toml uv.lock ./\n",
        "RUN uv pip install --system -r <(uv export --format requirements-txt)\n",
        "\n",
        "COPY lambda_function.py model.bin ./\n",
        "\n",
        "CMD [\"lambda_function.lambda_handler\"]\n",
        "```\n",
        "\n",
        "### Local Testing\n",
        "\n",
        "Build and run the Docker container locally:\n",
        "\n",
        "```bash\n",
        "docker build -t churn-prediction-lambda .\n",
        "docker run -it --rm -p 8080:8080 churn-prediction-lambda\n",
        "```\n",
        "\n",
        "Test with:\n",
        "```python\n",
        "import requests\n",
        "\n",
        "url = 'http://localhost:8080/2015-03-31/functions/function/invocations'\n",
        "customer = {...}  # customer data\n",
        "result = requests.post(url, json=customer).json()\n",
        "print(result)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: AWS Lambda Deployment\n",
        "\n",
        "### Creating ECR Repository\n",
        "\n",
        "First, create an Elastic Container Registry (ECR) repository:\n",
        "\n",
        "```bash\n",
        "aws ecr create-repository \\\n",
        "  --repository-name \"churn-prediction-lambda\" \\\n",
        "  --region \"eu-west-1\"\n",
        "```\n",
        "\n",
        "### Building and Pushing Docker Image\n",
        "\n",
        "```bash\n",
        "# Set your ECR URL (from the repository creation response)\n",
        "ECR_URL=\"YOUR_ACCOUNT_ID.dkr.ecr.eu-west-1.amazonaws.com\"\n",
        "\n",
        "# Login to ECR\n",
        "aws ecr get-login-password \\\n",
        "  --region \"eu-west-1\" \\\n",
        "  | docker login \\\n",
        "  --username AWS \\\n",
        "  --password-stdin ${ECR_URL}\n",
        "\n",
        "# Build, tag, and push\n",
        "REMOTE_IMAGE_TAG=\"${ECR_URL}/churn-prediction-lambda:v1\"\n",
        "\n",
        "docker build -t churn-prediction-lambda .\n",
        "docker tag churn-prediction-lambda ${REMOTE_IMAGE_TAG}\n",
        "docker push ${REMOTE_IMAGE_TAG}\n",
        "```\n",
        "\n",
        "### Creating Lambda Function\n",
        "\n",
        "1. Go to AWS Console → Lambda\n",
        "2. Create function → Container image\n",
        "3. Name: \"churn-prediction-docker\"\n",
        "4. Select your container image\n",
        "5. Create function\n",
        "6. Increase timeout to 30 seconds (Configuration → General Configuration → Edit)\n",
        "\n",
        "### Updating Lambda Function\n",
        "\n",
        "```bash\n",
        "REMOTE_IMAGE_TAG=\"${ECR_URL}/churn-prediction-lambda:v2\"\n",
        "\n",
        "docker build -t churn-prediction-lambda .\n",
        "docker tag churn-prediction-lambda ${REMOTE_IMAGE_TAG}\n",
        "docker push ${REMOTE_IMAGE_TAG}\n",
        "\n",
        "aws lambda update-function-code \\\n",
        "  --function-name churn-prediction-docker \\\n",
        "  --image-uri ${REMOTE_IMAGE_TAG} \\\n",
        "  --region eu-west-1\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: TensorFlow/Keras Models with ONNX\n",
        "\n",
        "Instead of TF-lite, we'll use ONNX (Open Neural Network Exchange) for deploying deep learning models. ONNX provides better compatibility and is easier to work with.\n",
        "\n",
        "### Downloading the Model\n",
        "\n",
        "```bash\n",
        "wget https://github.com/DataTalksClub/machine-learning-zoomcamp/releases/download/dl-models/clothing-model-new.keras\n",
        "```\n",
        "\n",
        "### Converting to ONNX\n",
        "\n",
        "The conversion happens in two steps:\n",
        "\n",
        "1. **Convert Keras model to TensorFlow SavedModel format**\n",
        "2. **Convert SavedModel to ONNX format**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Convert Keras model to SavedModel format\n",
        "from tensorflow import keras\n",
        "\n",
        "model = keras.models.load_model('clothing-model-new.keras')\n",
        "model.export(\"clothing-model-new_savedmodel\")\n",
        "\n",
        "print(\"Model converted to SavedModel format\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 2: Convert SavedModel to ONNX**\n",
        "\n",
        "This step should be done in a Docker container to avoid version conflicts. The command is:\n",
        "\n",
        "```bash\n",
        "python -m tf2onnx.convert \\\n",
        "    --saved-model clothing-model-new_savedmodel \\\n",
        "    --opset 13 \\\n",
        "    --output clothing-model-new.onnx\n",
        "```\n",
        "\n",
        "Or download the pre-converted model:\n",
        "\n",
        "```bash\n",
        "wget https://github.com/DataTalksClub/machine-learning-zoomcamp/releases/download/dl-models/clothing-model-new.onnx\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using ONNX Runtime\n",
        "\n",
        "Once we have the ONNX model, we can use ONNX Runtime for inference:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import onnxruntime as ort\n",
        "\n",
        "def load_onnx_model(model_path):\n",
        "    \"\"\"\n",
        "    Loads an ONNX model and returns the inference session and input/output names.\n",
        "    \n",
        "    Args:\n",
        "        model_path (str): Path to the ONNX model file.\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (session, input_name, output_name)\n",
        "    \"\"\"\n",
        "    session = ort.InferenceSession(model_path, providers=[\"CPUExecutionProvider\"])\n",
        "    \n",
        "    inputs = session.get_inputs()\n",
        "    outputs = session.get_outputs()\n",
        "    \n",
        "    input_name = inputs[0].name\n",
        "    output_name = outputs[0].name\n",
        "    \n",
        "    return session, input_name, output_name\n",
        "\n",
        "\n",
        "# Load the model (if available)\n",
        "# onnx_model_path = \"clothing-model-new.onnx\"\n",
        "# session, input_name, output_name = load_onnx_model(onnx_model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras_image_helper import create_preprocessor\n",
        "\n",
        "def create_image_preprocessor():\n",
        "    \"\"\"\n",
        "    Creates a preprocessor for Xception model input format.\n",
        "    \n",
        "    Returns:\n",
        "        Preprocessor object for image preprocessing.\n",
        "    \"\"\"\n",
        "    preprocessor = create_preprocessor('xception', target_size=(299, 299))\n",
        "    return preprocessor\n",
        "\n",
        "\n",
        "# Example usage (if model is available):\n",
        "# preprocessor = create_image_preprocessor()\n",
        "# url = 'http://bit.ly/mlbookcamp-pants'\n",
        "# X = preprocessor.from_url(url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_clothing_class(session, input_name, output_name, preprocessor, image_url):\n",
        "    \"\"\"\n",
        "    Makes a clothing classification prediction using ONNX model.\n",
        "    \n",
        "    Args:\n",
        "        session: ONNX Runtime inference session.\n",
        "        input_name (str): Name of the input tensor.\n",
        "        output_name (str): Name of the output tensor.\n",
        "        preprocessor: Image preprocessor object.\n",
        "        image_url (str): URL of the image to classify.\n",
        "    \n",
        "    Returns:\n",
        "        dict: Dictionary mapping class names to prediction probabilities.\n",
        "    \"\"\"\n",
        "    classes = [\n",
        "        'dress',\n",
        "        'hat',\n",
        "        'longsleeve',\n",
        "        'outwear',\n",
        "        'pants',\n",
        "        'shirt',\n",
        "        'shoes',\n",
        "        'shorts',\n",
        "        'skirt',\n",
        "        't-shirt'\n",
        "    ]\n",
        "    \n",
        "    X = preprocessor.from_url(image_url)\n",
        "    result = session.run([output_name], {input_name: X})\n",
        "    predictions = result[0][0].tolist()\n",
        "    \n",
        "    return dict(zip(classes, predictions))\n",
        "\n",
        "\n",
        "# Example usage (if model is available):\n",
        "# result = predict_clothing_class(session, input_name, output_name, preprocessor, url)\n",
        "# print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lambda Function for ONNX Model\n",
        "\n",
        "Here's the complete Lambda function for serving the ONNX model:\n",
        "\n",
        "```python\n",
        "import onnxruntime as ort\n",
        "from keras_image_helper import create_preprocessor\n",
        "\n",
        "preprocessor = create_preprocessor(\"xception\", target_size=(299, 299))\n",
        "\n",
        "session = ort.InferenceSession(\n",
        "    \"clothing-model-new.onnx\", providers=[\"CPUExecutionProvider\"]\n",
        ")\n",
        "input_name = session.get_inputs()[0].name\n",
        "output_name = session.get_outputs()[0].name\n",
        "\n",
        "classes = [\n",
        "    \"dress\",\n",
        "    \"hat\",\n",
        "    \"longsleeve\",\n",
        "    \"outwear\",\n",
        "    \"pants\",\n",
        "    \"shirt\",\n",
        "    \"shoes\",\n",
        "    \"shorts\",\n",
        "    \"skirt\",\n",
        "    \"t-shirt\",\n",
        "]\n",
        "\n",
        "\n",
        "def predict(url):\n",
        "    X = preprocessor.from_url(url)\n",
        "    result = session.run([output_name], {input_name: X})\n",
        "    float_predictions = result[0][0].tolist()\n",
        "    return dict(zip(classes, float_predictions))\n",
        "\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    url = event[\"url\"]\n",
        "    result = predict(url)\n",
        "    return result\n",
        "```\n",
        "\n",
        "**Dockerfile:**\n",
        "```dockerfile\n",
        "FROM public.ecr.aws/lambda/python:3.13\n",
        "\n",
        "RUN pip install onnxruntime keras-image-helper\n",
        "\n",
        "COPY clothing-model-new.onnx clothing-model-new.onnx\n",
        "COPY lambda_function.py ./\n",
        "\n",
        "CMD [\"lambda_function.lambda_handler\"]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: PyTorch Models with ONNX\n",
        "\n",
        "PyTorch models can also be converted to ONNX and served the same way. The main difference is in the preprocessing format.\n",
        "\n",
        "### Downloading PyTorch ONNX Model\n",
        "\n",
        "```bash\n",
        "wget https://github.com/DataTalksClub/machine-learning-zoomcamp/releases/download/dl-models/clothing_classifier_mobilenet_v2_latest.onnx\n",
        "```\n",
        "\n",
        "### PyTorch Preprocessing\n",
        "\n",
        "PyTorch uses a different image format (NCHW instead of NHWC):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras_image_helper import create_preprocessor\n",
        "\n",
        "\n",
        "def preprocess_pytorch(X):\n",
        "    \"\"\"\n",
        "    Preprocesses images for PyTorch models (NCHW format).\n",
        "    \n",
        "    Args:\n",
        "        X: Input image array with shape (1, 299, 299, 3), dtype=float32, values in [0, 255]\n",
        "    \n",
        "    Returns:\n",
        "        np.ndarray: Preprocessed image in NCHW format (batch, channels, height, width)\n",
        "    \"\"\"\n",
        "    # Normalize to [0, 1]\n",
        "    X = X / 255.0\n",
        "\n",
        "    # ImageNet normalization constants\n",
        "    mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3, 1, 1)\n",
        "    std = np.array([0.229, 0.224, 0.225]).reshape(1, 3, 1, 1)\n",
        "\n",
        "    # Convert NHWC → NCHW\n",
        "    # from (batch, height, width, channels) → (batch, channels, height, width)\n",
        "    X = X.transpose(0, 3, 1, 2)\n",
        "\n",
        "    # Normalize\n",
        "    X = (X - mean) / std\n",
        "\n",
        "    return X.astype(np.float32)\n",
        "\n",
        "\n",
        "# Create preprocessor with PyTorch preprocessing\n",
        "preprocessor_pytorch = create_preprocessor(preprocess_pytorch, target_size=(224, 224))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lambda Function for PyTorch ONNX Model\n",
        "\n",
        "The Lambda function structure is similar, but uses the PyTorch preprocessor:\n",
        "\n",
        "```python\n",
        "import onnxruntime as ort\n",
        "from keras_image_helper import create_preprocessor\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_pytorch(X):\n",
        "    X = X / 255.0\n",
        "    mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3, 1, 1)\n",
        "    std = np.array([0.229, 0.224, 0.225]).reshape(1, 3, 1, 1)\n",
        "    X = X.transpose(0, 3, 1, 2)\n",
        "    X = (X - mean) / std\n",
        "    return X.astype(np.float32)\n",
        "\n",
        "preprocessor = create_preprocessor(preprocess_pytorch, target_size=(224, 224))\n",
        "\n",
        "session = ort.InferenceSession(\n",
        "    \"clothing_classifier_mobilenet_v2_latest.onnx\", \n",
        "    providers=[\"CPUExecutionProvider\"]\n",
        ")\n",
        "input_name = session.get_inputs()[0].name\n",
        "output_name = session.get_outputs()[0].name\n",
        "\n",
        "classes = [\n",
        "    \"dress\", \"hat\", \"longsleeve\", \"outwear\", \"pants\",\n",
        "    \"shirt\", \"shoes\", \"shorts\", \"skirt\", \"t-shirt\"\n",
        "]\n",
        "\n",
        "def predict(url):\n",
        "    X = preprocessor.from_url(url)\n",
        "    result = session.run([output_name], {input_name: X})\n",
        "    float_predictions = result[0][0].tolist()\n",
        "    return dict(zip(classes, float_predictions))\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    url = event[\"url\"]\n",
        "    result = predict(url)\n",
        "    return result\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook covered:\n",
        "\n",
        "1. **Scikit-Learn Model Deployment**\n",
        "   - Training a churn prediction model\n",
        "   - Basic AWS Lambda function creation\n",
        "   - Docker containerization to overcome size limitations\n",
        "   - ECR deployment and AWS Lambda container image deployment\n",
        "\n",
        "2. **Deep Learning Model Deployment with ONNX**\n",
        "   - Converting TensorFlow/Keras models to ONNX format\n",
        "   - Converting PyTorch models to ONNX format\n",
        "   - Using ONNX Runtime for efficient inference\n",
        "   - Docker-based deployment for deep learning models\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- **Docker solves size limitations**: Scikit-Learn dependencies exceed Lambda's ZIP limit\n",
        "- **ONNX is a better alternative to TF-lite**: Easier to work with and better compatibility\n",
        "- **Local testing is crucial**: Test Docker containers locally before deploying\n",
        "- **ONNX Runtime is lightweight**: Only need ONNX Runtime, not full TensorFlow/PyTorch\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Explore API Gateway integration for web service exposure\n",
        "- Implement monitoring and logging with CloudWatch\n",
        "- Consider using AWS Step Functions for complex ML workflows\n",
        "- Explore other serverless services like AWS Batch for training\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
