{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine Learning and Deep Learning Model Deployment with Serverless\n",
        "\n",
        "This notebook covers deploying ML models using AWS Lambda and Docker, including:\n",
        "- Scikit-Learn model deployment\n",
        "- Deep Learning models with ONNX (TensorFlow/Keras and PyTorch)\n",
        "\n",
        "**Video**: https://www.youtube.com/watch?v=sHQaeVm5hT8\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- AWS Account\n",
        "- AWS CLI installed and configured\n",
        "- Docker installed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Training a Scikit-Learn Model\n",
        "\n",
        "First, we'll train a simple churn prediction model using Scikit-Learn. This model will be deployed to AWS Lambda in the following sections.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "\n",
        "print(f'pandas=={pd.__version__}')\n",
        "print(f'sklearn=={sklearn.__version__}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def load_data():\n",
        "#     \"\"\"\n",
        "#     Loads and preprocesses the Telco customer churn dataset.\n",
        "    \n",
        "#     Returns:\n",
        "#         pd.DataFrame: Preprocessed dataframe with cleaned column names and data types.\n",
        "#     \"\"\"\n",
        "#     data_url = 'https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-03-churn-prediction/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
        "\n",
        "#     df = pd.read_csv(data_url)\n",
        "\n",
        "#     # Normalize column names\n",
        "#     df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
        "\n",
        "#     # Normalize categorical values\n",
        "#     categorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n",
        "\n",
        "#     for column in categorical_columns:\n",
        "#         df[column] = df[column].str.lower().str.replace(' ', '_')\n",
        "\n",
        "#     # Handle numeric conversion and missing values\n",
        "#     df.totalcharges = pd.to_numeric(df.totalcharges, errors='coerce')\n",
        "#     df.totalcharges = df.totalcharges.fillna(0)\n",
        "\n",
        "#     # Convert churn to binary\n",
        "#     df.churn = (df.churn == 'yes').astype(int)\n",
        "    \n",
        "#     return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def train_model(df):\n",
        "#     \"\"\"\n",
        "#     Trains a logistic regression model for churn prediction.\n",
        "    \n",
        "#     Args:\n",
        "#         df (pd.DataFrame): Preprocessed dataframe with features and target.\n",
        "    \n",
        "#     Returns:\n",
        "#         sklearn.pipeline.Pipeline: Trained pipeline with DictVectorizer and LogisticRegression.\n",
        "#     \"\"\"\n",
        "#     numerical_features = ['tenure', 'monthlycharges', 'totalcharges']\n",
        "\n",
        "#     categorical_features = [\n",
        "#         'gender',\n",
        "#         'seniorcitizen',\n",
        "#         'partner',\n",
        "#         'dependents',\n",
        "#         'phoneservice',\n",
        "#         'multiplelines',\n",
        "#         'internetservice',\n",
        "#         'onlinesecurity',\n",
        "#         'onlinebackup',\n",
        "#         'deviceprotection',\n",
        "#         'techsupport',\n",
        "#         'streamingtv',\n",
        "#         'streamingmovies',\n",
        "#         'contract',\n",
        "#         'paperlessbilling',\n",
        "#         'paymentmethod',\n",
        "#     ]\n",
        "\n",
        "#     y_train = df.churn\n",
        "#     train_dict = df[categorical_features + numerical_features].to_dict(orient='records')\n",
        "\n",
        "#     pipeline = make_pipeline(\n",
        "#         DictVectorizer(),\n",
        "#         LogisticRegression(solver='liblinear')\n",
        "#     )\n",
        "\n",
        "#     pipeline.fit(train_dict, y_train)\n",
        "\n",
        "#     return pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def save_model(pipeline, output_file):\n",
        "#     \"\"\"\n",
        "#     Saves a trained pipeline to disk using pickle.\n",
        "    \n",
        "#     Args:\n",
        "#         pipeline: Trained sklearn pipeline to save.\n",
        "#         output_file (str): Path where the model will be saved.\n",
        "#     \"\"\"\n",
        "#     with open(output_file, 'wb') as f_out:\n",
        "#         pickle.dump(pipeline, f_out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Load data and train model\n",
        "# df = load_data()\n",
        "# pipeline = train_model(df)\n",
        "# save_model(pipeline, 'model.bin')\n",
        "\n",
        "# print('Model saved to model.bin')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: AWS Lambda Basics\n",
        "\n",
        "AWS Lambda is a serverless compute service that runs code in response to events. Let's start with a simple Lambda function that returns a mock prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Invoking the Lambda Function\n",
        "\n",
        "Once deployed, you can invoke the Lambda function in several ways:\n",
        "\n",
        "#### Method 1: AWS CLI\n",
        "\n",
        "First, create a JSON file with the customer data (e.g., `customer.json`):\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"customer\": {\n",
        "    \"gender\": \"female\",\n",
        "    \"seniorcitizen\": 0,\n",
        "    \"partner\": \"yes\",\n",
        "    \"dependents\": \"no\",\n",
        "    \"phoneservice\": \"no\",\n",
        "    \"multiplelines\": \"no_phone_service\",\n",
        "    \"internetservice\": \"dsl\",\n",
        "    \"onlinesecurity\": \"no\",\n",
        "    \"onlinebackup\": \"yes\",\n",
        "    \"deviceprotection\": \"no\",\n",
        "    \"techsupport\": \"no\",\n",
        "    \"streamingtv\": \"no\",\n",
        "    \"streamingmovies\": \"no\",\n",
        "    \"contract\": \"month-to-month\",\n",
        "    \"paperlessbilling\": \"yes\",\n",
        "    \"paymentmethod\": \"electronic_check\",\n",
        "    \"tenure\": 1,\n",
        "    \"monthlycharges\": 29.85,\n",
        "    \"totalcharges\": 29.85\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "Then invoke the function:\n",
        "\n",
        "```bash\n",
        "aws lambda invoke --function-name churn_prediction --cli-binary-format raw-in-base64-out --payload file://customer.json --region us-west-2 output.json && cat output.json\n",
        "```\n",
        "\n",
        "**Note:** Make sure to specify the `--region` parameter matching the region where your Lambda function is deployed. If you haven't configured AWS CLI defaults, you can also set the region using `aws configure` or by setting the `AWS_DEFAULT_REGION` environment variable.\n",
        "\n",
        "The response will be saved to `output.json`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Method 2: Using boto3 (Python)\n",
        "\n",
        "You can also invoke the Lambda function programmatically using boto3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Using `aws login` credentials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code below uses `aws login` credentials. For a simpler alternative, use `aws configure` instead - then boto3 will work automatically without credential loading code. See [troubleshooting guide](aws-docs/troubleshooting.md) for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import boto3\n",
        "# import json\n",
        "# import os\n",
        "# from pathlib import Path\n",
        "\n",
        "\n",
        "# def load_aws_login_credentials():\n",
        "#     \"\"\"\n",
        "#     Loads credentials from aws login cache.\n",
        "    \n",
        "#     Returns:\n",
        "#         dict: Credentials dict with access_key, secret_key, token, or None if not found.\n",
        "#     \"\"\"\n",
        "#     login_cache_dir = Path.home() / '.aws' / 'login' / 'cache'\n",
        "#     credential_files = list(login_cache_dir.glob('*.json'))\n",
        "    \n",
        "#     if not credential_files:\n",
        "#         return None\n",
        "    \n",
        "#     try:\n",
        "#         with open(credential_files[0]) as f:\n",
        "#             creds_data = json.load(f)\n",
        "#             access_token = creds_data.get('accessToken', {})\n",
        "#             return {\n",
        "#                 'aws_access_key_id': access_token.get('accessKeyId'),\n",
        "#                 'aws_secret_access_key': access_token.get('secretAccessKey'),\n",
        "#                 'aws_session_token': access_token.get('sessionToken')\n",
        "#             }\n",
        "#     except Exception:\n",
        "#         return None\n",
        "\n",
        "\n",
        "# # Load credentials once at module level\n",
        "# _aws_creds = load_aws_login_credentials()\n",
        "# if _aws_creds:\n",
        "#     os.environ.update(_aws_creds)\n",
        "#     print(\"✓ AWS credentials loaded from aws login cache\")\n",
        "# else:\n",
        "#     print(\"⚠ Warning: No credentials found in aws login cache. Make sure you've run 'aws login'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def invoke_lambda_function(function_name, payload, region_name='us-west-2'):\n",
        "#     \"\"\"\n",
        "#     Invokes an AWS Lambda function.\n",
        "    \n",
        "#     Args:\n",
        "#         function_name (str): Lambda function name.\n",
        "#         payload (dict): Request payload.\n",
        "#         region_name (str): AWS region. Defaults to 'us-west-2'.\n",
        "    \n",
        "#     Returns:\n",
        "#         dict: Lambda function response.\n",
        "#     \"\"\"\n",
        "#     if _aws_creds:\n",
        "#         session = boto3.Session(\n",
        "#             aws_access_key_id=_aws_creds['aws_access_key_id'],\n",
        "#             aws_secret_access_key=_aws_creds['aws_secret_access_key'],\n",
        "#             aws_session_token=_aws_creds['aws_session_token'],\n",
        "#             region_name=region_name\n",
        "#         )\n",
        "#     else:\n",
        "#         session = boto3.Session(region_name=region_name)\n",
        "    \n",
        "#     lambda_client = session.client('lambda', region_name=region_name)\n",
        "#     response = lambda_client.invoke(\n",
        "#         FunctionName=function_name,\n",
        "#         InvocationType='RequestResponse',\n",
        "#         Payload=json.dumps(payload)\n",
        "#     )\n",
        "#     return json.loads(response['Payload'].read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Alternative: Using `aws configure` (Simpler)\n",
        "\n",
        "If you use `aws configure` instead of `aws login`, boto3 works automatically without credential loading code:\n",
        "\n",
        "**Setup:** Run `aws configure` in terminal, then boto3 automatically uses credentials from `~/.aws/credentials`. See [troubleshooting guide](aws-docs/troubleshooting.md) for details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import boto3\n",
        "import json\n",
        "\n",
        "def invoke_lambda_function(function_name, payload, region_name='us-east-1'):\n",
        "    \"\"\"\n",
        "    Invokes an AWS Lambda function.\n",
        "    \n",
        "    Args:\n",
        "        function_name (str): Lambda function name.\n",
        "        payload (dict): Request payload.\n",
        "        region_name (str): AWS region. Defaults to 'us-west-2'.\n",
        "    \n",
        "    Returns:\n",
        "        dict: Lambda function response.\n",
        "    \"\"\"\n",
        "    lambda_client = boto3.client('lambda', region_name=region_name)\n",
        "    response = lambda_client.invoke(\n",
        "        FunctionName=function_name,\n",
        "        InvocationType='RequestResponse',\n",
        "        Payload=json.dumps(payload)\n",
        "    )\n",
        "    return json.loads(response['Payload'].read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "customer_data = {\n",
        "    \"customer\": {\n",
        "        \"gender\": \"female\",\n",
        "        \"seniorcitizen\": 0,\n",
        "        \"partner\": \"yes\",\n",
        "        \"dependents\": \"no\",\n",
        "        \"phoneservice\": \"no\",\n",
        "        \"multiplelines\": \"no_phone_service\",\n",
        "        \"internetservice\": \"dsl\",\n",
        "        \"onlinesecurity\": \"no\",\n",
        "        \"onlinebackup\": \"yes\",\n",
        "        \"deviceprotection\": \"no\",\n",
        "        \"techsupport\": \"no\",\n",
        "        \"streamingtv\": \"no\",\n",
        "        \"streamingmovies\": \"no\",\n",
        "        \"contract\": \"month-to-month\",\n",
        "        \"paperlessbilling\": \"yes\",\n",
        "        \"paymentmethod\": \"electronic_check\",\n",
        "        \"tenure\": 1,\n",
        "        \"monthlycharges\": 29.85,\n",
        "        \"totalcharges\": 29.85\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = invoke_lambda_function('churn_prediction', customer_data, region_name='us-east-1')\n",
        "print(json.dumps(result, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** You can also expose the Lambda function as a web service using API Gateway. See [unit 9.7 about API Gateway](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/09-serverless/07-api-gateway.md) for more details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: AWS Lambda with Docker: Running Locally\n",
        "\n",
        "Scikit-Learn and its dependencies exceed the 250MB ZIP archive limit for Lambda. Docker containers solve this problem.\n",
        "\n",
        "We'll set up the Lambda function using UV for dependency management, which provides a modern and efficient way to handle Python packages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading the Model and Lambda Function Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs('lambda-sklearn', exist_ok=True)  # Create directory if it doesn't exist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile lambda-sklearn/lambda_function.py\n",
        "import pickle\n",
        "\n",
        "with open('model.bin', 'rb') as f_in:\n",
        "    pipeline = pickle.load(f_in)\n",
        "\n",
        "def predict_single(customer):\n",
        "    result = pipeline.predict_proba(customer)[0, 1]\n",
        "    return float(result)\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    customer = event['customer']\n",
        "    prediction = predict_single(customer)\n",
        "    \n",
        "    return {\n",
        "        'churn_probability': prediction,\n",
        "        'churn': prediction >= 0.5\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting Up Dependencies with UV\n",
        "\n",
        "We'll use UV (a fast Python package manager) to manage dependencies systematically:\n",
        "\n",
        "1. Create `requirements.in` - centralized dependency list\n",
        "2. Initialize UV project - creates `pyproject.toml`\n",
        "3. Add dependencies to `pyproject.toml` from `requirements.in`\n",
        "4. Generate lock file - creates `uv.lock` for reproducible builds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile lambda-sklearn/requirements.in\n",
        "pandas\n",
        "scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize UV project (creates pyproject.toml)\n",
        "!cd lambda-sklearn && uv init --name churn-prediction-lambda --no-readme\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add all dependencies from requirements.in to pyproject.toml\n",
        "!cd lambda-sklearn && uv add $(grep -v '^#' requirements.in | xargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate lock file\n",
        "!cd lambda-sklearn && uv lock"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dockerfile\n",
        "\n",
        "The Dockerfile uses the AWS Lambda Python base image and UV to install dependencies:\n",
        "\n",
        "- Base image: `public.ecr.aws/lambda/python:3.13`\n",
        "- UV package manager: Copied from official UV Docker image\n",
        "- Dependencies: Installed from `pyproject.toml` and `uv.lock` using UV\n",
        "- Application files: `lambda_function.py` and `model.bin`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile lambda-sklearn/Dockerfile\n",
        "\n",
        "# Use the official AWS Lambda base image for Python 3.13\n",
        "FROM public.ecr.aws/lambda/python:3.13\n",
        "\n",
        "# # Instead of installing 'uv' via curl/pip, we copy the binary directly from its official Docker image\n",
        "COPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/\n",
        "\n",
        "# Copy dependency definition files into the container\n",
        "COPY pyproject.toml uv.lock ./\n",
        "\n",
        "# Install dependencies system-wide inside the Lambda image\n",
        "# AWS Lambda does NOT use virtual environments, so packages must go into the system site-packages\n",
        "# We run a single command that does two things:\n",
        "# a. 'uv export': Converts 'uv.lock' into a requirements.txt format in memory.\n",
        "# b. 'uv pip install': Installs those requirements. 'uv pip install' installs them—like pip, but faster.\n",
        "# The '--system' flag tells uv to install libraries globally (into /var/lang/lib/python3.13)\n",
        "RUN uv pip install --system -r <(uv export --format requirements-txt)\n",
        "\n",
        "# Copy the application code into the container\n",
        "COPY lambda_function.py model.bin ./\n",
        "\n",
        "# Set the entry point to the lambda_handler function\n",
        "CMD [\"lambda_function.lambda_handler\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE:\n",
        "\n",
        "```text\n",
        "[ EXTERNAL REGISTRIES ]                        [ LOCAL PROJECT ]\n",
        "                 |                                     |\n",
        " 1. BASE IMAGE   |                                     |\n",
        " +----------------------------------+                  |\n",
        " |  public.ecr.aws/lambda/python    |                  |\n",
        " |  (OS + Python 3.13 Runtime)      |                  |\n",
        " +---------------+------------------+                  |\n",
        "                 |                                     |\n",
        " 2. INJECT TOOLS | (Multi-stage)                       |\n",
        " +---------------v------------------+                  |\n",
        " | COPY /uv binary from ghcr.io...  |                  |\n",
        " | (Fast Python package manager)    |                  |\n",
        " +---------------+------------------+                  |\n",
        "                 |                                     |\n",
        " 3. DEPENDENCIES |                                     |\n",
        " +---------------v------------------+      +-----------v-----------+\n",
        " | COPY pyproject.toml & uv.lock    |<-----|  Dependency Files     |\n",
        " | RUN uv pip install --system ...  |      +-----------+-----------+\n",
        " | (Installs libs directly to OS)   |                  |\n",
        " +---------------+------------------+                  |\n",
        "                 |                                     |\n",
        " 4. APP CODE     |                                     |\n",
        " +---------------v------------------+      +-----------v-----------+\n",
        " | COPY lambda_function.py          |<-----|  Source Code & Model  |\n",
        " | COPY model.bin                   |      +-----------------------+\n",
        " +---------------+------------------+\n",
        "                 |\n",
        " 5. ENTRY POINT  |\n",
        " +---------------v------------------+\n",
        " | CMD [lambda_handler]             |\n",
        " | (Waits for AWS Invoke events)    |\n",
        " +----------------------------------+\n",
        "                 |\n",
        "        [ FINAL DOCKER IMAGE ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build Docker container locally\n",
        "!cd lambda-sklearn && docker build -t churn-prediction-lambda ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Local Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To run the Docker container locally, execute this command in your terminal:\n",
        "# docker run -it --rm -p 8080:8080 churn-prediction-lambda\n",
        "#\n",
        "# Note: This is an interactive command that runs in the foreground.\n",
        "# Run it in a separate terminal/bash session, not in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = 'http://localhost:8080/2015-03-31/functions/function/invocations'\n",
        "\n",
        "result = requests.post(url, json=customer_data).json()\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: AWS Lambda Deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating ECR Repository\n",
        "\n",
        "**Why:** ECR stores Docker images that Lambda pulls to run your function.\n",
        "\n",
        "Create an Elastic Container Registry (ECR) repository:\n",
        "\n",
        "```bash\n",
        "aws ecr create-repository \\\n",
        "  --repository-name \"churn-prediction-lambda\" \\\n",
        "  --region \"us-east-1\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building and Pushing Docker Image\n",
        "\n",
        "**Why:** Lambda needs the container image in ECR to deploy the function.\n",
        "\n",
        "You can build and push the Docker image manually using these commands:\n",
        "\n",
        "```bash\n",
        "# Set your ECR URL (from the repository creation response)\n",
        "ECR_URL=\"YOUR_ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com\"\n",
        "\n",
        "# Login to ECR\n",
        "aws ecr get-login-password \\\n",
        "  --region \"us-east-1\" \\\n",
        "  | docker login \\\n",
        "  --username AWS \\\n",
        "  --password-stdin ${ECR_URL}\n",
        "\n",
        "# Build, tag, and push\n",
        "REMOTE_IMAGE_TAG=\"${ECR_URL}/churn-prediction-lambda:v1\"\n",
        "\n",
        "docker build -t churn-prediction-lambda .\n",
        "docker tag churn-prediction-lambda ${REMOTE_IMAGE_TAG}\n",
        "docker push ${REMOTE_IMAGE_TAG}\n",
        "```\n",
        "\n",
        "**However, this approach has limitations:**\n",
        "- Requires manually setting the ECR URL each time\n",
        "- Hard-coded values (region, repository name) that can be forgotten\n",
        "- Repetitive commands that are error-prone\n",
        "- No version management for image tags\n",
        "\n",
        "**Better approach: Automated Script**\n",
        "\n",
        "The `publish.sh` script (created in the cell below) automates this process and follow the principles:\n",
        "\n",
        "- **Avoid Hard-Coded Values**: Uses named constants for configuration\n",
        "- **Reusability**: Can be run with different versions: `./publish.sh 1`, `./publish.sh 2`, etc.\n",
        "- **Error Handling**: Includes `set -e` to exit on errors\n",
        "- **Auto-detection**: Automatically gets AWS account ID\n",
        "\n",
        "This makes the deployment process more reliable, maintainable, and less error-prone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile lambda-sklearn/publish.sh\n",
        "#!/bin/bash\n",
        "# Publish Docker image to AWS ECR\n",
        "# \n",
        "# This script builds, tags, and pushes the Docker image to ECR.\n",
        "# It follows clean code principles with meaningful variable names\n",
        "# and avoids hard-coded values.\n",
        "\n",
        "set -e  # Exit on error\n",
        "\n",
        "# Configuration constants\n",
        "readonly DEFAULT_REGION=\"us-east-1\"\n",
        "readonly REPOSITORY_NAME=\"churn-prediction-lambda\"\n",
        "readonly IMAGE_TAG_PREFIX=\"v\"\n",
        "readonly LOCAL_IMAGE_NAME=\"churn-prediction-lambda\"\n",
        "\n",
        "# Get AWS account ID and construct ECR URL\n",
        "AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\n",
        "ECR_BASE_URL=\"${AWS_ACCOUNT_ID}.dkr.ecr.${DEFAULT_REGION}.amazonaws.com\"\n",
        "ECR_REPOSITORY_URI=\"${ECR_BASE_URL}/${REPOSITORY_NAME}\"\n",
        "\n",
        "# Generate image tag with version (defaults to v1 if not provided)\n",
        "IMAGE_VERSION=\"${1:-1}\"\n",
        "REMOTE_IMAGE_TAG=\"${ECR_REPOSITORY_URI}:${IMAGE_TAG_PREFIX}${IMAGE_VERSION}\"\n",
        "\n",
        "echo \"Building Docker image: ${LOCAL_IMAGE_NAME}\"\n",
        "docker build -t \"${LOCAL_IMAGE_NAME}\" .\n",
        "\n",
        "echo \"Tagging image: ${REMOTE_IMAGE_TAG}\"\n",
        "docker tag \"${LOCAL_IMAGE_NAME}\" \"${REMOTE_IMAGE_TAG}\"\n",
        "\n",
        "echo \"Authenticating Docker to ECR...\"\n",
        "aws ecr get-login-password --region \"${DEFAULT_REGION}\" \\\n",
        "  | docker login --username AWS --password-stdin \"${ECR_BASE_URL}\"\n",
        "\n",
        "echo \"Pushing image to ECR: ${REMOTE_IMAGE_TAG}\"\n",
        "docker push \"${REMOTE_IMAGE_TAG}\"\n",
        "\n",
        "echo \"✓ Successfully published ${REMOTE_IMAGE_TAG}\"\n",
        "echo \"Image URI: ${REMOTE_IMAGE_TAG}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make the script executable\n",
        "!chmod +x lambda-sklearn/publish.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Usage:**\n",
        "\n",
        "```bash\n",
        "cd lambda-sklearn\n",
        "./publish.sh        # Publishes as v1\n",
        "./publish.sh 2      # Publishes as v2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Lambda Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Using AWS Console\n",
        "\n",
        "1. Go to AWS Console → Lambda\n",
        "2. Create function → Container image\n",
        "3. Name: \"churn-prediction-docker\"\n",
        "4. Select your container image\n",
        "5. Create function\n",
        "6. Increase timeout to 30 seconds (Configuration → General Configuration → Edit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Using AWS CLI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Create IAM Role (if needed)\n",
        "\n",
        "Lambda needs an IAM role to access AWS services (e.g., CloudWatch for logging).\n",
        "\n",
        "**Requires IAM permissions.** If the role doesn't exist:\n",
        "\n",
        "```bash\n",
        "# Create trust policy - allows Lambda service to assume this role\n",
        "cat > trust-policy.json <<EOF\n",
        "{\n",
        "  \"Version\": \"2012-10-17\",\n",
        "  \"Statement\": [\n",
        "    {\n",
        "      \"Effect\": \"Allow\",\n",
        "      \"Principal\": {\n",
        "        \"Service\": \"lambda.amazonaws.com\"\n",
        "      },\n",
        "      \"Action\": \"sts:AssumeRole\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "EOF\n",
        "\n",
        "# Create role - defines who can use it\n",
        "aws iam create-role \\\n",
        "  --role-name lambda-execution-role \\\n",
        "  --assume-role-policy-document file://trust-policy.json\n",
        "\n",
        "# Attach execution policy - grants CloudWatch logging permissions\n",
        "aws iam attach-role-policy \\\n",
        "  --role-name lambda-execution-role \\\n",
        "  --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Get IAM Role ARN\n",
        "\n",
        "Lambda function creation requires the role ARN to assign permissions.\n",
        "\n",
        "Get the ARN of an existing Lambda execution role:\n",
        "\n",
        "```bash\n",
        "ROLE_ARN=$(aws iam get-role --role-name lambda-execution-role --query 'Role.Arn' --output text)\n",
        "echo \"Role ARN: $ROLE_ARN\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Get ECR Image URI\n",
        "\n",
        "Lambda needs the full ECR image URI to pull the container image.\n",
        "\n",
        "Construct the ECR image URI after publishing:\n",
        "\n",
        "```bash\n",
        "AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\n",
        "ECR_URI=\"${AWS_ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/churn-prediction-lambda:v1\"\n",
        "echo \"ECR URI: $ECR_URI\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Create Lambda Function\n",
        "\n",
        "Create the Lambda function with the container image:\n",
        "\n",
        "```bash\n",
        "aws lambda create-function \\\n",
        "  --function-name churn-prediction-docker \\\n",
        "  --package-type Image \\\n",
        "  --code ImageUri=\"${ECR_URI}\" \\\n",
        "  --role \"${ROLE_ARN}\" \\\n",
        "  --timeout 30 \\\n",
        "  --memory-size 512 \\\n",
        "  --region us-east-1\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Invoking the Lambda Function: Test the deployed function and integrate it into applications\n",
        "\n",
        "After creating the Lambda function, you can invoke it programmatically using Python and boto3.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `invoke.py` script demonstrates:\n",
        "\n",
        "1. **Initialize boto3 Lambda client** - Creates a client to interact with AWS Lambda\n",
        "2. **Prepare customer data** - Formats input in the expected structure\n",
        "3. **Invoke the function** - Calls Lambda with customer data\n",
        "4. **Process the response** - Parses and displays prediction results\n",
        "\n",
        "**Key parameters:**\n",
        "- `FunctionName`: Specifies which Lambda function to invoke\n",
        "- `InvocationType='RequestResponse'`: Synchronous invocation (waits for response)\n",
        "- `Payload`: JSON-encoded input data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile lambda-sklearn/invoke.py\n",
        "import boto3\n",
        "import json\n",
        "\n",
        "lambda_client = boto3.client('lambda')\n",
        "\n",
        "customer_data = {\n",
        "    \"customer\": {\n",
        "        \"gender\": \"female\",\n",
        "        \"seniorcitizen\": 0,\n",
        "        \"partner\": \"yes\",\n",
        "        \"dependents\": \"no\",\n",
        "        \"phoneservice\": \"no\",\n",
        "        \"multiplelines\": \"no_phone_service\",\n",
        "        \"internetservice\": \"dsl\",\n",
        "        \"onlinesecurity\": \"no\",\n",
        "        \"onlinebackup\": \"yes\",\n",
        "        \"deviceprotection\": \"no\",\n",
        "        \"techsupport\": \"no\",\n",
        "        \"streamingtv\": \"no\",\n",
        "        \"streamingmovies\": \"no\",\n",
        "        \"contract\": \"month-to-month\",\n",
        "        \"paperlessbilling\": \"yes\",\n",
        "        \"paymentmethod\": \"electronic_check\",\n",
        "        \"tenure\": 1,\n",
        "        \"monthlycharges\": 29.85,\n",
        "        \"totalcharges\": 29.85\n",
        "    }\n",
        "}\n",
        "\n",
        "response = lambda_client.invoke(\n",
        "    FunctionName='churn-prediction-docker',\n",
        "    InvocationType='RequestResponse',\n",
        "    Payload=json.dumps(customer_data)\n",
        ")\n",
        "\n",
        "result = json.loads(response['Payload'].read())\n",
        "print(json.dumps(result, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Updating Lambda Function\n",
        "\n",
        "Deploy new code changes without recreating the function.\n",
        "\n",
        "```bash\n",
        "# Build and push new image version\n",
        "REMOTE_IMAGE_TAG=\"${ECR_URL}/churn-prediction-lambda:v2\"\n",
        "\n",
        "docker build -t churn-prediction-lambda .\n",
        "docker tag churn-prediction-lambda ${REMOTE_IMAGE_TAG}\n",
        "docker push ${REMOTE_IMAGE_TAG}\n",
        "\n",
        "# Update function to use new image\n",
        "aws lambda update-function-code \\\n",
        "  --function-name churn-prediction-docker \\\n",
        "  --image-uri ${REMOTE_IMAGE_TAG} \\\n",
        "  --region us-east-1\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: TensorFlow/Keras Models with ONNX\n",
        "\n",
        "Previously we used TF-lite for AWS Lambda. In this workshop, we'll use an alternative - ONNX (Open Neural Network Exchange).\n",
        "\n",
        "We'll use the same Keras model as before. It was retrained for the newest TF version. You can see the [training process here](https://colab.research.google.com/drive/1GTkGkq1QKOtAL0wiMjYr-LOCpVgsAwyk?usp=sharing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Downloading the Model\n",
        "\n",
        "Create the directory and download the Keras model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Create lambda-keras directory\n",
        "os.makedirs('lambda-keras', exist_ok=True)\n",
        "\n",
        "# Download the Keras model\n",
        "!cd lambda-keras && wget https://github.com/DataTalksClub/machine-learning-zoomcamp/releases/download/dl-models/clothing-model-new.keras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Converting to ONNX\n",
        "\n",
        "The conversion happens in two steps:\n",
        "\n",
        "1. **Convert Keras model to TensorFlow SavedModel format**\n",
        "2. **Convert SavedModel to ONNX format**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### STEP 1: Convert Keras model to TensorFlow SavedModel format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "print(\"Python executable:\", sys.executable)\n",
        "print(\"Python path:\", sys.path[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install TensorFlow (required for model conversion)\n",
        "# Note: After installation, restart the kernel for the import to work\n",
        "%pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Convert Keras model to SavedModel format\n",
        "# Note: Requires TensorFlow installed and kernel restarted after installation\n",
        "# Alternative: Skip this step and download the pre-converted ONNX model (see Step 2)\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "model = keras.models.load_model('lambda-keras/clothing-model-new.keras')\n",
        "model.export(\"lambda-keras/clothing-model-new_savedmodel\")\n",
        "\n",
        "print(\"Model converted to SavedModel format\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### STEP 2: Convert SavedModel to ONNX\n",
        "\n",
        "**Recommended:** Download the pre-converted ONNX model (avoids TensorFlow installation issues):\n",
        "\n",
        "```bash\n",
        "cd lambda-keras\n",
        "wget https://github.com/DataTalksClub/machine-learning-zoomcamp/releases/download/dl-models/clothing-model-new.onnx\n",
        "```\n",
        "\n",
        "**Alternative:** Convert manually. First install tf2onnx (install from GitHub as the latest release doesn't support numpy 2):\n",
        "\n",
        "```bash\n",
        "pip install git+https://github.com/onnx/tensorflow-onnx.git\n",
        "```\n",
        "\n",
        "Then convert:\n",
        "\n",
        "```bash\n",
        "python -m tf2onnx.convert \\\n",
        "    --saved-model lambda-keras/clothing-model-new_savedmodel \\\n",
        "    --opset 13 \\\n",
        "    --output lambda-keras/clothing-model-new.onnx\n",
        "```\n",
        "\n",
        "**Note:** To avoid version conflicts between TensorFlow and ONNX, consider using Docker for conversion (see README.md for Docker-based conversion steps)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Manual Conversion: Install tf2onnx\n",
        "\n",
        "Install tf2onnx from GitHub (latest release doesn't support numpy 2):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install tf2onnx from GitHub\n",
        "%pip install git+https://github.com/onnx/tensorflow-onnx.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Manual Conversion: Convert SavedModel to ONNX\n",
        "\n",
        "**⚠️ Version Conflict Warning:** tf2onnx may install an incompatible protobuf version. If you encounter import errors, use Docker for conversion (see README.md) or download the pre-converted model.\n",
        "\n",
        "Convert the SavedModel format to ONNX format:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert SavedModel to ONNX\n",
        "!python -m tf2onnx.convert \\\n",
        "    --saved-model lambda-keras/clothing-model-new_savedmodel \\\n",
        "    --opset 13 \\\n",
        "    --output lambda-keras/clothing-model-new.onnx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Download Pre-converted Model (Recommended)\n",
        "\n",
        "If you prefer to skip the conversion, download the pre-converted ONNX model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download pre-converted ONNX model (recommended - avoids TensorFlow installation)\n",
        "!cd lambda-keras && wget https://github.com/DataTalksClub/machine-learning-zoomcamp/releases/download/dl-models/clothing-model-new.onnx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using ONNX Runtime\n",
        "\n",
        "Now our models are saved in the ONNX format. Like with TF-lite, we only need ONNX-Runtime to run it.\n",
        "\n",
        "Like in the module, we can't use TF for our preprocessing. That's why we will rely on `keras-image-helper` to do that.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install onnxruntime\n",
        "%pip install keras_image_helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Using ONNX Runtime for inference\n",
        "import onnxruntime as ort\n",
        "from keras_image_helper import create_preprocessor\n",
        "\n",
        "# Load ONNX model\n",
        "onnx_model_path = \"lambda-keras/clothing-model-new.onnx\"\n",
        "session = ort.InferenceSession(onnx_model_path, providers=[\"CPUExecutionProvider\"])\n",
        "input_name = session.get_inputs()[0].name\n",
        "output_name = session.get_outputs()[0].name\n",
        "\n",
        "# Get the image\n",
        "preprocessor = create_preprocessor('xception', target_size=(299, 299))\n",
        "url = 'http://bit.ly/mlbookcamp-pants'\n",
        "X = preprocessor.from_url(url)\n",
        "\n",
        "# Make predictions\n",
        "result = session.run([output_name], {input_name: X})\n",
        "predictions = result[0][0].tolist()\n",
        "\n",
        "classes = ['dress', 'hat', 'longsleeve', 'outwear', 'pants', 'shirt', 'shoes', 'shorts', 'skirt', 't-shirt']\n",
        "dict(zip(classes, predictions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lambda Function for ONNX Model\n",
        "\n",
        "This Lambda function uses ONNX Runtime to serve the converted Keras model. It loads the ONNX model, preprocesses images using `keras-image-helper`, and returns predictions for clothing categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile lambda-keras/lambda_function.py\n",
        "import onnxruntime as ort\n",
        "from keras_image_helper import create_preprocessor\n",
        "\n",
        "preprocessor = create_preprocessor(\"xception\", target_size=(299, 299))\n",
        "\n",
        "session = ort.InferenceSession(\n",
        "    \"clothing-model-new.onnx\", providers=[\"CPUExecutionProvider\"]\n",
        ")\n",
        "input_name = session.get_inputs()[0].name\n",
        "output_name = session.get_outputs()[0].name\n",
        "\n",
        "classes = [\n",
        "    \"dress\",\n",
        "    \"hat\",\n",
        "    \"longsleeve\",\n",
        "    \"outwear\",\n",
        "    \"pants\",\n",
        "    \"shirt\",\n",
        "    \"shoes\",\n",
        "    \"shorts\",\n",
        "    \"skirt\",\n",
        "    \"t-shirt\",\n",
        "]\n",
        "\n",
        "\n",
        "def predict(url):\n",
        "    X = preprocessor.from_url(url)\n",
        "    result = session.run([output_name], {input_name: X})\n",
        "    float_predictions = result[0][0].tolist()\n",
        "    return dict(zip(classes, float_predictions))\n",
        "\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    url = event[\"url\"]\n",
        "    result = predict(url)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dockerizing the Lambda Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile lambda-keras/Dockerfile\n",
        "FROM public.ecr.aws/lambda/python:3.13\n",
        "\n",
        "RUN pip install onnxruntime keras-image-helper\n",
        "\n",
        "COPY clothing-model-new.onnx clothing-model-new.onnx\n",
        "COPY lambda_function.py ./\n",
        "\n",
        "CMD [\"lambda_function.lambda_handler\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building and Running the Docker Container\n",
        "\n",
        "Build the Docker image:\n",
        "\n",
        "```bash\n",
        "docker build -t clothing-lambda-keras lambda-keras\n",
        "```\n",
        "\n",
        "Run it locally:\n",
        "\n",
        "```bash\n",
        "docker run -it --rm -p 8080:8080 clothing-lambda-keras\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing the Lambda Function\n",
        "\n",
        "Test the function locally:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = 'http://localhost:8080/2015-03-31/functions/function/invocations'\n",
        "\n",
        "request = {\n",
        "    \"url\": \"http://bit.ly/mlbookcamp-pants\"\n",
        "}\n",
        "\n",
        "result = requests.post(url, json=request).json()\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: PyTorch Models with ONNX\n",
        "\n",
        "With PyTorch, we can do the same:\n",
        "- Convert a model to ONNX\n",
        "- Serve it with the same code as before\n",
        "\n",
        "Here's a [model we trained with PyTorch](https://colab.research.google.com/drive/1_kvvbi_msBuTFkkdLxMEpB3mj-Jhh-Bc?usp=sharing).\n",
        "\n",
        "In PyTorch, when we train a model, we can save it directly to ONNX:\n",
        "\n",
        "```python\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    input,\n",
        "    onnx_path,\n",
        "    ...\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Downloading Pre-converted PyTorch ONNX Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download PyTorch ONNX model\n",
        "import os\n",
        "os.makedirs('lambda-pytorch', exist_ok=True)\n",
        "\n",
        "!cd lambda-pytorch && wget https://github.com/DataTalksClub/machine-learning-zoomcamp/releases/download/dl-models/clothing_classifier_mobilenet_v2_latest.onnx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lambda Function for PyTorch ONNX Model\n",
        "\n",
        "This Lambda function uses ONNX Runtime to serve the PyTorch model. It uses a custom preprocessor for PyTorch's image format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile lambda-pytorch/lambda_function.py\n",
        "import onnxruntime as ort\n",
        "from keras_image_helper import create_preprocessor\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_pytorch(X):\n",
        "    X = X / 255.0\n",
        "    mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3, 1, 1)\n",
        "    std = np.array([0.229, 0.224, 0.225]).reshape(1, 3, 1, 1)\n",
        "    X = X.transpose(0, 3, 1, 2)\n",
        "    X = (X - mean) / std\n",
        "    return X.astype(np.float32)\n",
        "\n",
        "preprocessor = create_preprocessor(preprocess_pytorch, target_size=(224, 224))\n",
        "\n",
        "session = ort.InferenceSession(\n",
        "    \"clothing_classifier_mobilenet_v2_latest.onnx\", \n",
        "    providers=[\"CPUExecutionProvider\"]\n",
        ")\n",
        "input_name = session.get_inputs()[0].name\n",
        "output_name = session.get_outputs()[0].name\n",
        "\n",
        "classes = [\n",
        "    \"dress\", \"hat\", \"longsleeve\", \"outwear\", \"pants\",\n",
        "    \"shirt\", \"shoes\", \"shorts\", \"skirt\", \"t-shirt\"\n",
        "]\n",
        "\n",
        "def predict(url):\n",
        "    X = preprocessor.from_url(url)\n",
        "    result = session.run([output_name], {input_name: X})\n",
        "    float_predictions = result[0][0].tolist()\n",
        "    return dict(zip(classes, float_predictions))\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    url = event[\"url\"]\n",
        "    result = predict(url)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dockerizing the Lambda Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile lambda-pytorch/Dockerfile\n",
        "FROM public.ecr.aws/lambda/python:3.13\n",
        "\n",
        "RUN pip install onnxruntime keras-image-helper\n",
        "\n",
        "COPY clothing_classifier_mobilenet_v2_latest.onnx clothing_classifier_mobilenet_v2_latest.onnx\n",
        "COPY lambda_function.py ./\n",
        "\n",
        "CMD [\"lambda_function.lambda_handler\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building and Running the Docker Container\n",
        "\n",
        "Build the Docker image:\n",
        "\n",
        "```bash\n",
        "docker build -t clothing-lambda-onnx lambda-pytorch\n",
        "```\n",
        "\n",
        "Run it locally:\n",
        "\n",
        "```bash\n",
        "docker run -it --rm -p 8080:8080 clothing-lambda-onnx\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing the Lambda Function\n",
        "\n",
        "Test the function locally:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = 'http://localhost:8080/2015-03-31/functions/function/invocations'\n",
        "\n",
        "request = {\n",
        "    \"url\": \"http://bit.ly/mlbookcamp-pants\"\n",
        "}\n",
        "\n",
        "result = requests.post(url, json=request).json()\n",
        "print(result)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
